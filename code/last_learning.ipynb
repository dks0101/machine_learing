{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1x5Q9GZGuseWtwrl0xmrkAeUh2wTpKZyQ",
      "authorship_tag": "ABX9TyOUq/kcMsv9CH7QyETGDRY2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dks0101/machine_learing/blob/main/code/last_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrVdxVDupSB3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import chardet\n",
        "torch.manual_seed(0)\n",
        "\n",
        "seq_length = 24\n",
        "data_dim = 1\n",
        "hidden_dim = 10\n",
        "output_dim = 24\n",
        "learning_rate = 0.06\n",
        "iterations = 350\n",
        "#train data 입력\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/기후 train/data_tr_city.csv', encoding='euc-kr')\n",
        "train_df.columns = ['datetime', 'D']\n",
        "#test data 입력\n",
        "with open('/content/drive/MyDrive/기후 train/data_ts_city.csv', 'rb') as f:\n",
        "    result = chardet.detect(f.read())\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/기후 train/data_ts_city.csv', encoding=result['encoding'], error_bad_lines=False)\n",
        "test_df.columns = ['datetime', 'D']\n",
        "#데이터 전처리\n",
        "#'D'에서 1000보다 큰 값들을 찾고 값을 na으로 만들어주기\n",
        "train_df.loc[train_df['D'] > 1000, 'D'] = np.NaN\n",
        "#이상치 선형 보간 - 총 3개의 이상치\n",
        "index = train_df[train_df.D == max(train_df.D)].index\n",
        "train_df.D[index] = np.NaN\n",
        "index = train_df[train_df.D == min(train_df.D)].index\n",
        "train_df.D[index] = np.NaN\n",
        "# 앞서 min(data_tr.적산차)인 행을 nan값으로 변경했으므로 그 다음 min값이 리턴된다.\n",
        "index = train_df[train_df.D== min(train_df.D)].index\n",
        "train_df.D[index] = np.NaN\n",
        "# 이상치를 NaN으로 대체한 이후에 linear 보간을 수행 & 결측치도 linear보간 수행\n",
        "train_df['D'] = train_df['D'].interpolate(method='linear')\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "# Min-Max Scaling을 위한 객체 생성\n",
        "scaler = MinMaxScaler()\n",
        "# 'D' 열 선택 및 2차원 배열로 변환\n",
        "D = train_df['D'].values.reshape(-1, 1)\n",
        "# Min-Max Scaling 적용\n",
        "scaled_D = scaler.fit_transform(D)\n",
        "# 정규화된 값을 다시 'D' 열에 할당\n",
        "train_df['D'] = scaled_D\n",
        "# 'D' 열 선택 및 2차원 배열로 변환\n",
        "test_D = test_df['D'].values.reshape(-1, 1)\n",
        "# 학습 데이터의 스케일러 객체를 사용하여 테스트 데이터에도 변환 적용\n",
        "scaled_test_D = scaler.transform(test_D)\n",
        "# 정규화된 값을 다시 'D' 열에 할당\n",
        "test_df['D'] = scaled_test_D\n",
        "# datetime을 index로 지정\n",
        "train_df = train_df.set_index('datetime')\n",
        "test_df = test_df.set_index('datetime')\n",
        "# 시계열 데이터 생성 함수\n",
        "def build_dataset(time_series, seq_length):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(0, len(time_series) - seq_length):\n",
        "        _x = time_series.values[i:i+seq_length, :]\n",
        "        _y = time_series.values[i+seq_length, [-1]]\n",
        "        dataX.append(_x)\n",
        "        dataY.append(_y)\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "trainX, trainY = build_dataset(train_df, seq_length)\n",
        "testX, testY = build_dataset(test_df, seq_length)\n",
        "\n",
        "trainX_tensor = torch.FloatTensor(trainX)\n",
        "trainY_tensor = torch.FloatTensor(trainY)\n",
        "\n",
        "testX_tensor = torch.FloatTensor(testX)\n",
        "testY_tensor = torch.FloatTensor(testY)\n",
        "#model\n",
        "class Net(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim, layers):\n",
        "    super (Net, self).__init__()\n",
        "    self.rnn = torch.nn. LSTM(input_dim, hidden_dim, num_layers=layers, batch_first=True)\n",
        "    self.fc= torch.nn. Linear(hidden_dim, output_dim, bias=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x, _status = self.rnn(x)\n",
        "    x = self.fc(x[:, -1])\n",
        "    return x\n",
        "\n",
        "net = Net(data_dim, hidden_dim, output_dim, 1)\n",
        "\n",
        "# loss & optimizer setting\n",
        "criterion = torch.nn.L1Loss()\n",
        "optimizer = optim.Adam (net.parameters(), lr=learning_rate)\n",
        "\n",
        "for i in range(iterations):\n",
        "    optimizer.zero_grad()\n",
        "    outputs = net(trainX_tensor)\n",
        "    loss = criterion(outputs, trainY_tensor)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # MAE Loss 값 출력\n",
        "    scaled_loss = loss.item()\n",
        "    original_loss = scaler.inverse_transform([[scaled_loss]])[0][0]\n",
        "\n",
        "    # MAE 계산\n",
        "    mae = torch.mean(torch.abs(outputs - trainY_tensor))\n",
        "    scaled_mae = mae.item()\n",
        "    original_mae = scaler.inverse_transform([[scaled_mae]])[0][0]\n",
        "\n",
        "    test_outputs = net(testX_tensor)\n",
        "    test_loss = criterion(test_outputs, testY_tensor)\n",
        "    scaled_test_loss = test_loss.item()\n",
        "    original_test_loss = scaler.inverse_transform([[scaled_test_loss]])[0][0]\n",
        "\n",
        "    test_mae = torch.mean(torch.abs(test_outputs - testY_tensor))\n",
        "    scaled_test_mae = test_mae.item()\n",
        "    original_test_mae = scaler.inverse_transform([[scaled_test_mae]])[0][0]\n",
        "    print(\"Iteration:\", i+1, \" Train Loss:\", original_loss, \"Train MAE:\", original_mae, \"Test Loss:\", original_test_loss, \"Test MAE:\", original_test_mae)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#그래프로 결과 출력\n",
        "plt.figure(figsize=(200, 10))\n",
        "original_testY = scaler.inverse_transform(testY_tensor.numpy())\n",
        "plt.plot(original_testY)\n",
        "prediction = scaler.inverse_transform(net(testX_tensor).data.numpy())\n",
        "plt.plot(prediction)\n",
        "plt.legend(['original', 'prediction'])\n",
        "plt.show()\n",
        "\n",
        "# DataFrame 생성\n",
        "df = pd.DataFrame(prediction)\n",
        "# DataFrame 출력\n",
        "with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
        "    print(df.tail(5))\n"
      ],
      "metadata": {
        "id": "_q00F1tXvwVy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}